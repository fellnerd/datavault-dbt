# Data Vault 2.1 - Developer Guide

> **Projekt:** Virtual Data Vault 2.1 auf Azure  
> **Version:** 2.0.0  
> **Stand:** 2025-12-27  
> **Zielgruppe:** Entwickler, Data Engineers

---

## ğŸ“‘ Inhaltsverzeichnis

1. [Quick Reference](#-quick-reference)
2. [Projektstruktur](#-projektstruktur)
3. [Neues Attribut hinzufÃ¼gen](#-neues-attribut-hinzufÃ¼gen)
4. [Neue Entity erstellen (Komplett)](#-neue-entity-erstellen-komplett)
5. [Einzelne Objekte erstellen](#-einzelne-objekte-erstellen)
   - [Hub erstellen](#51-hub-erstellen)
   - [Satellite erstellen](#52-satellite-erstellen)
   - [Link erstellen](#53-link-erstellen)
   - [Reference Table erstellen](#54-reference-table-erstellen)
   - [Effectivity Satellite erstellen](#55-effectivity-satellite-erstellen)
   - [PIT Table erstellen](#56-pit-table-erstellen)
6. [Mart View erstellen](#-mart-view-erstellen)
7. [Tests hinzufÃ¼gen](#-tests-hinzufÃ¼gen)
8. [Deployment Workflow](#-deployment-workflow)
9. [Troubleshooting](#-troubleshooting)
10. [Checklisten](#-checklisten)

---

## ğŸš€ Quick Reference

### HÃ¤ufigste Befehle

```bash
# Umgebung aktivieren
cd ~/projects/datavault-dbt && source .venv/bin/activate

# Verbindung testen
dbt debug

# Models bauen
dbt run                              # Alle Models
dbt run --select hub_company         # Einzelnes Model
dbt run --select +sat_company+       # Model mit AbhÃ¤ngigkeiten
dbt run --full-refresh               # Alles neu bauen

# External Tables aktualisieren
dbt run-operation stage_external_sources

# Tests
dbt test                             # Alle Tests
dbt test --select hub_company        # Tests fÃ¼r ein Model

# Seeds (Reference Data)
dbt seed                             # Alle Seeds laden

# Kompilieren (SQL anzeigen ohne AusfÃ¼hrung)
dbt compile --select model_name
cat target/compiled/datavault/models/path/to/model.sql
```

### Wichtige Dateien

| Datei | Zweck | Link |
|-------|-------|------|
| `dbt_project.yml` | Projektkonfiguration | [Ã¶ffnen](../dbt_project.yml) |
| `models/staging/sources.yml` | External Tables Definition | [Ã¶ffnen](../models/staging/sources.yml) |
| `models/schema.yml` | Tests & Dokumentation | [Ã¶ffnen](../models/schema.yml) |
| `macros/generate_schema_name.sql` | Schema-Naming | [Ã¶ffnen](../macros/generate_schema_name.sql) |
| `macros/satellite_current_flag.sql` | Current Flag Macro | [Ã¶ffnen](../macros/satellite_current_flag.sql) |
| `macros/ghost_records.sql` | Ghost Records | [Ã¶ffnen](../macros/ghost_records.sql) |

---

## ğŸ“ Projektstruktur

```
datavault-dbt/
â”œâ”€â”€ dbt_project.yml              # âš™ï¸ Projektkonfiguration
â”œâ”€â”€ packages.yml                 # ğŸ“¦ Package-AbhÃ¤ngigkeiten
â”œâ”€â”€ profiles.yml                 # ğŸ” In ~/.dbt/ (nicht im Repo!)
â”‚
â”œâ”€â”€ macros/                      # ğŸ”§ Wiederverwendbare Macros
â”‚   â”œâ”€â”€ generate_schema_name.sql
â”‚   â”œâ”€â”€ satellite_current_flag.sql
â”‚   â””â”€â”€ ghost_records.sql
â”‚
â”œâ”€â”€ seeds/                       # ğŸŒ± Reference Data (CSV)
â”‚   â””â”€â”€ ref_role.csv
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ schema.yml              # ğŸ“‹ Tests & Dokumentation
â”‚   â”‚
â”‚   â”œâ”€â”€ staging/                # ğŸ“¥ Staging Layer
â”‚   â”‚   â”œâ”€â”€ sources.yml         #    External Table Definitionen
â”‚   â”‚   â”œâ”€â”€ stg_company.sql     #    Staging View
â”‚   â”‚   â””â”€â”€ stg_country.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ raw_vault/              # ğŸ›ï¸ Raw Vault Layer
â”‚   â”‚   â”œâ”€â”€ hubs/
â”‚   â”‚   â”‚   â”œâ”€â”€ hub_company.sql
â”‚   â”‚   â”‚   â””â”€â”€ hub_country.sql
â”‚   â”‚   â”œâ”€â”€ satellites/
â”‚   â”‚   â”‚   â”œâ”€â”€ sat_company.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ sat_country.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ sat_company_client_ext.sql
â”‚   â”‚   â”‚   â””â”€â”€ eff_sat_company_country.sql
â”‚   â”‚   â””â”€â”€ links/
â”‚   â”‚       â”œâ”€â”€ link_company_role.sql
â”‚   â”‚       â””â”€â”€ link_company_country.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ business_vault/         # ğŸ“Š Business Vault Layer
â”‚   â”‚   â””â”€â”€ pit_company.sql
â”‚   â”‚
â”‚   â””â”€â”€ mart/                   # ğŸ“ˆ Mart Layer (fÃ¼r BI)
â”‚       â””â”€â”€ (Views fÃ¼r Reporting)
â”‚
â”œâ”€â”€ docs/                       # ğŸ“š Dokumentation
â”‚   â”œâ”€â”€ SYSTEM.md
â”‚   â”œâ”€â”€ USER.md
â”‚   â”œâ”€â”€ DEVELOPER.md            # â† Diese Datei
â”‚   â””â”€â”€ MODEL_ARCHITECTURE.md
â”‚
â””â”€â”€ target/                     # ğŸ¯ Kompilierte Artefakte
    â””â”€â”€ compiled/               #    Generiertes SQL
```

---

## â• Neues Attribut hinzufÃ¼gen

### Szenario
Ein bestehendes Attribut soll zum Satellite hinzugefÃ¼gt werden (z.B. `tax_number` zu `sat_company`).

### Schritt-fÃ¼r-Schritt

#### Schritt 1: External Table erweitern

ğŸ“„ **Datei:** [models/staging/sources.yml](../models/staging/sources.yml)

```yaml
# Finde die External Table und fÃ¼ge die Spalte hinzu
- name: ext_company_client
  columns:
    # ... bestehende Spalten ...
    - name: tax_number          # â† NEU
      data_type: NVARCHAR(50)   # â† Datentyp
```

#### Schritt 2: Staging View erweitern

ğŸ“„ **Datei:** [models/staging/stg_company.sql](../models/staging/stg_company.sql)

```sql
-- 1. FÃ¼ge Spalte zur SELECT-Liste hinzu
client_source AS (
    SELECT 
        object_id,
        -- ... bestehende Spalten ...
        tax_number,              -- â† NEU
        -- ...
    FROM {{ source('staging', 'ext_company_client') }}
),

-- 2. Falls im Hash Diff: FÃ¼ge zur hashdiff_columns Liste hinzu
{%- set hashdiff_columns = [
    'name',
    'street',
    -- ... bestehende ...
    'tax_number'                 -- â† NEU (falls Ã„nderungen getrackt werden sollen)
] -%}
```

#### Schritt 3: Satellite erweitern

ğŸ“„ **Datei:** [models/raw_vault/satellites/sat_company.sql](../models/raw_vault/satellites/sat_company.sql)

```sql
WITH source_data AS (
    SELECT 
        hk_company,
        hd_company,
        -- ... bestehende Spalten ...
        tax_number,              -- â† NEU
        dss_load_date,
        dss_record_source
    FROM {{ ref('stg_company') }}
    WHERE hk_company IS NOT NULL
),
-- ... Rest bleibt gleich ...
```

#### Schritt 4: Deployment

```bash
# External Table aktualisieren
dbt run-operation stage_external_sources

# Satellite neu bauen (full-refresh wegen SchemaÃ¤nderung!)
dbt run --full-refresh --select stg_company sat_company

# Tests ausfÃ¼hren
dbt test --select sat_company
```

### âš ï¸ Wichtig
- Bei **Schema-Ã„nderungen** immer `--full-refresh` verwenden
- Hash Diff nur erweitern wenn Ã„nderungen getrackt werden sollen
- Nach Ã„nderung: Tests ausfÃ¼hren!

---

## ğŸ—ï¸ Neue Entity erstellen (Komplett)

### Szenario
Eine komplett neue Entity soll ins Data Vault (z.B. `product` aus einer neuen Quelltabelle).

### Ãœbersicht der Schritte

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. External Table    â†’  2. Staging View  â†’  3. Hub             â”‚
â”‚        â†“                                          â†“              â”‚
â”‚  sources.yml               stg_product.sql      hub_product.sql â”‚
â”‚                                   â†“                    â†“         â”‚
â”‚                            4. Satellite         5. Link          â”‚
â”‚                            sat_product.sql      link_*.sql       â”‚
â”‚                                   â†“                              â”‚
â”‚                            6. Tests & Deploy                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Schritt 1: External Table definieren

ğŸ“„ **Datei:** [models/staging/sources.yml](../models/staging/sources.yml)

```yaml
sources:
  - name: staging
    database: "{{ target.database }}"
    schema: stg
    tables:
      # ... bestehende Tabellen ...
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # NEU: Product
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - name: ext_product
        external:
          location: "werkportal/postgres/public.wp_product.parquet"
          file_format: ParquetFormat
        columns:
          - name: object_id
            data_type: BIGINT
            tests:
              - not_null
          - name: name
            data_type: NVARCHAR(255)
          - name: description
            data_type: NVARCHAR(MAX)
          - name: price
            data_type: DECIMAL(18,2)
          - name: category_id
            data_type: BIGINT
          - name: dss_record_source
            data_type: NVARCHAR(100)
          - name: dss_load_date
            data_type: DATETIME2
          - name: dss_run_id
            data_type: NVARCHAR(100)
```

### Schritt 2: Staging View erstellen

ğŸ“„ **Neue Datei:** `models/staging/stg_product.sql`

```sql
/*
 * Staging Model: stg_product
 * 
 * Bereitet Product-Daten fÃ¼r das Data Vault vor.
 * Hash Key Separator: '^^' (DV 2.1 Standard)
 */

{%- set hashdiff_columns = [
    'name',
    'description',
    'price',
    'category_id'
] -%}

WITH source AS (
    SELECT * FROM {{ source('staging', 'ext_product') }}
),

staged AS (
    SELECT
        -- ===========================================
        -- HASH KEYS
        -- ===========================================
        CONVERT(CHAR(64), HASHBYTES('SHA2_256', 
            ISNULL(CAST(object_id AS NVARCHAR(MAX)), '')
        ), 2) AS hk_product,
        
        -- FK zu anderen Hubs (falls vorhanden)
        CONVERT(CHAR(64), HASHBYTES('SHA2_256', 
            ISNULL(CAST(category_id AS NVARCHAR(MAX)), '')
        ), 2) AS hk_category,
        
        -- ===========================================
        -- HASH DIFF (Change Detection)
        -- ===========================================
        CONVERT(CHAR(64), HASHBYTES('SHA2_256', 
            CONCAT(
                {%- for col in hashdiff_columns %}
                ISNULL(CAST({{ col }} AS NVARCHAR(MAX)), ''){{ ',' if not loop.last else '' }}
                {%- endfor %}
            )
        ), 2) AS hd_product,
        
        -- ===========================================
        -- BUSINESS KEY
        -- ===========================================
        object_id,
        
        -- ===========================================
        -- PAYLOAD
        -- ===========================================
        name,
        description,
        price,
        category_id,
        
        -- ===========================================
        -- METADATA
        -- ===========================================
        COALESCE(dss_record_source, 'werkportal') AS dss_record_source,
        COALESCE(TRY_CAST(dss_load_date AS DATETIME2), GETDATE()) AS dss_load_date,
        dss_run_id
        
    FROM source
)

SELECT * FROM staged
```

### Schritt 3: Hub erstellen

ğŸ“„ **Neue Datei:** `models/raw_vault/hubs/hub_product.sql`

```sql
/*
 * Hub: hub_product
 * Schema: vault
 * 
 * Speichert eindeutige Product Business Keys.
 * Insert-Only: Neue Products werden hinzugefÃ¼gt, nie gelÃ¶scht.
 */

{{ config(
    materialized='incremental',
    unique_key='hk_product',
    as_columnstore=false
) }}

WITH source_data AS (
    SELECT DISTINCT
        hk_product,
        object_id,
        dss_load_date,
        dss_record_source
    FROM {{ ref('stg_product') }}
    WHERE hk_product IS NOT NULL
),

{% if is_incremental() %}
existing_hubs AS (
    SELECT hk_product FROM {{ this }}
),
{% endif %}

new_records AS (
    SELECT
        src.hk_product,
        src.object_id,
        src.dss_load_date,
        src.dss_record_source
    FROM source_data src
    {% if is_incremental() %}
    WHERE NOT EXISTS (
        SELECT 1 FROM existing_hubs eh
        WHERE eh.hk_product = src.hk_product
    )
    {% endif %}
)

SELECT * FROM new_records
```

### Schritt 4: Satellite erstellen

ğŸ“„ **Neue Datei:** `models/raw_vault/satellites/sat_product.sql`

```sql
/*
 * Satellite: sat_product
 * Schema: vault
 * 
 * Speichert Product-Attribute mit vollstÃ¤ndiger Historie.
 * dss_is_current: 'Y' fÃ¼r aktuellen Eintrag
 * dss_end_date: Ende der GÃ¼ltigkeit
 */

{{ config(
    materialized='incremental',
    unique_key='hk_product',
    as_columnstore=false,
    post_hook=[
        "{{ update_satellite_current_flag(this, 'hk_product') }}"
    ]
) }}

WITH source_data AS (
    SELECT 
        hk_product,
        hd_product,
        dss_load_date,
        dss_record_source,
        -- Payload
        name,
        description,
        price,
        category_id
    FROM {{ ref('stg_product') }}
    WHERE hk_product IS NOT NULL
),

{% if is_incremental() %}
existing_sats AS (
    SELECT 
        hk_product,
        hd_product
    FROM {{ this }}
),
{% endif %}

new_records AS (
    SELECT
        src.hk_product,
        src.hd_product,
        src.dss_load_date,
        src.dss_record_source,
        src.name,
        src.description,
        src.price,
        src.category_id
    FROM source_data src
    {% if is_incremental() %}
    WHERE NOT EXISTS (
        SELECT 1 FROM existing_sats es
        WHERE es.hk_product = src.hk_product
          AND es.hd_product = src.hd_product
    )
    {% endif %}
)

SELECT 
    *,
    'Y' AS dss_is_current,
    CAST(NULL AS DATETIME2) AS dss_end_date
FROM new_records
```

### Schritt 5: Tests hinzufÃ¼gen

ğŸ“„ **Datei:** [models/schema.yml](../models/schema.yml)

```yaml
models:
  # ... bestehende Models ...
  
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Product
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  - name: stg_product
    columns:
      - name: hk_product
        tests:
          - not_null
      - name: object_id
        tests:
          - not_null

  - name: hub_product
    columns:
      - name: hk_product
        tests:
          - unique
          - not_null
      - name: object_id
        tests:
          - not_null
      - name: dss_load_date
        tests:
          - not_null
      - name: dss_record_source
        tests:
          - not_null

  - name: sat_product
    columns:
      - name: hk_product
        tests:
          - not_null
          - relationships:
              to: ref('hub_product')
              field: hk_product
      - name: hd_product
        tests:
          - not_null
```

### Schritt 6: Deployment

```bash
# 1. External Table erstellen
dbt run-operation stage_external_sources

# 2. Alle neuen Models bauen
dbt run --select stg_product hub_product sat_product

# 3. Tests ausfÃ¼hren
dbt test --select stg_product hub_product sat_product

# 4. Ghost Records hinzufÃ¼gen (optional)
# â†’ Macro in ghost_records.sql erweitern
```

---

## ğŸ”¨ Einzelne Objekte erstellen

### 5.1 Hub erstellen

ğŸ“„ **Vorlage:** [models/raw_vault/hubs/hub_company.sql](../models/raw_vault/hubs/hub_company.sql)

**Minimales Template:**

```sql
{{ config(
    materialized='incremental',
    unique_key='hk_<entity>',
    as_columnstore=false
) }}

WITH source_data AS (
    SELECT DISTINCT
        hk_<entity>,
        <business_key_columns>,
        dss_load_date,
        dss_record_source
    FROM {{ ref('stg_<entity>') }}
    WHERE hk_<entity> IS NOT NULL
),

{% if is_incremental() %}
existing_hubs AS (
    SELECT hk_<entity> FROM {{ this }}
),
{% endif %}

new_records AS (
    SELECT *
    FROM source_data src
    {% if is_incremental() %}
    WHERE NOT EXISTS (
        SELECT 1 FROM existing_hubs eh
        WHERE eh.hk_<entity> = src.hk_<entity>
    )
    {% endif %}
)

SELECT * FROM new_records
```

**Ersetzen:**
- `<entity>` â†’ Name der Entity (z.B. `product`)
- `<business_key_columns>` â†’ Spalten des Business Keys

---

### 5.2 Satellite erstellen

ğŸ“„ **Vorlage:** [models/raw_vault/satellites/sat_company.sql](../models/raw_vault/satellites/sat_company.sql)

**Minimales Template:**

```sql
{{ config(
    materialized='incremental',
    unique_key='hk_<entity>',
    as_columnstore=false,
    post_hook=[
        "{{ update_satellite_current_flag(this, 'hk_<entity>') }}"
    ]
) }}

WITH source_data AS (
    SELECT 
        hk_<entity>,
        hd_<entity>,
        dss_load_date,
        dss_record_source,
        -- Payload Spalten hier
        <payload_columns>
    FROM {{ ref('stg_<entity>') }}
    WHERE hk_<entity> IS NOT NULL
),

{% if is_incremental() %}
existing_sats AS (
    SELECT hk_<entity>, hd_<entity> FROM {{ this }}
),
{% endif %}

new_records AS (
    SELECT *
    FROM source_data src
    {% if is_incremental() %}
    WHERE NOT EXISTS (
        SELECT 1 FROM existing_sats es
        WHERE es.hk_<entity> = src.hk_<entity>
          AND es.hd_<entity> = src.hd_<entity>
    )
    {% endif %}
)

SELECT 
    *,
    'Y' AS dss_is_current,
    CAST(NULL AS DATETIME2) AS dss_end_date
FROM new_records
```

---

### 5.3 Link erstellen

ğŸ“„ **Vorlage:** [models/raw_vault/links/link_company_role.sql](../models/raw_vault/links/link_company_role.sql)

**Minimales Template:**

```sql
{{ config(
    materialized='incremental',
    unique_key='hk_link_<entity1>_<entity2>',
    as_columnstore=false
) }}

WITH source_data AS (
    SELECT DISTINCT
        hk_link_<entity1>_<entity2>,
        hk_<entity1>,
        hk_<entity2>,
        dss_load_date,
        dss_record_source
    FROM {{ ref('stg_<source>') }}
    WHERE hk_<entity1> IS NOT NULL
      AND hk_<entity2> IS NOT NULL
),

{% if is_incremental() %}
existing_links AS (
    SELECT hk_link_<entity1>_<entity2> FROM {{ this }}
),
{% endif %}

new_records AS (
    SELECT *
    FROM source_data src
    {% if is_incremental() %}
    WHERE NOT EXISTS (
        SELECT 1 FROM existing_links el
        WHERE el.hk_link_<entity1>_<entity2> = src.hk_link_<entity1>_<entity2>
    )
    {% endif %}
)

SELECT * FROM new_records
```

**Wichtig:** Der Link Hash Key muss im Staging berechnet werden:

```sql
-- In stg_<source>.sql
CONVERT(CHAR(64), HASHBYTES('SHA2_256', 
    CONCAT(
        ISNULL(CAST(<entity1_bk> AS NVARCHAR(MAX)), ''),
        '^^',
        ISNULL(CAST(<entity2_bk> AS NVARCHAR(MAX)), '')
    )
), 2) AS hk_link_<entity1>_<entity2>
```

---

### 5.4 Reference Table erstellen

ğŸ“„ **Vorlage:** [seeds/ref_role.csv](../seeds/ref_role.csv)

**Schritt 1:** CSV-Datei erstellen

```csv
role_code,role_name,role_description
CLIENT,Kunde,Unternehmen das Dienstleistungen bezieht
CONTRACTOR,Auftragnehmer,Unternehmen das AuftrÃ¤ge ausfÃ¼hrt
SUPPLIER,Lieferant,Unternehmen das Waren liefert
```

ğŸ“„ **Speichern als:** `seeds/ref_<name>.csv`

**Schritt 2:** Konfiguration in dbt_project.yml

ğŸ“„ **Datei:** [dbt_project.yml](../dbt_project.yml)

```yaml
seeds:
  datavault:
    +schema: vault
    ref_<name>:
      +column_types:
        <column>: VARCHAR(50)
```

**Schritt 3:** Deployment

```bash
dbt seed --select ref_<name>
```

---

### 5.5 Effectivity Satellite erstellen

ğŸ“„ **Vorlage:** [models/raw_vault/satellites/eff_sat_company_country.sql](../models/raw_vault/satellites/eff_sat_company_country.sql)

FÃ¼r Links die **GÃ¼ltigkeitszeitrÃ¤ume** haben (z.B. "Firma war von 2020-2023 in Deutschland").

```sql
{{ config(
    materialized='incremental',
    unique_key=['hk_<hub>', 'dss_start_date'],
    as_columnstore=false,
    post_hook=[
        "{{ update_effectivity_end_dates() }}"
    ]
) }}

WITH source_data AS (
    SELECT
        hk_<hub>,
        hk_<related_hub>,
        hk_link_<entity1>_<entity2>,
        dss_load_date AS dss_start_date,
        dss_record_source
    FROM {{ ref('stg_<source>') }}
),

{% if is_incremental() %}
existing AS (
    SELECT hk_<hub>, dss_start_date FROM {{ this }}
),
{% endif %}

new_records AS (
    SELECT *
    FROM source_data src
    {% if is_incremental() %}
    WHERE NOT EXISTS (
        SELECT 1 FROM existing e
        WHERE e.hk_<hub> = src.hk_<hub>
          AND e.dss_start_date = src.dss_start_date
    )
    {% endif %}
)

SELECT 
    *,
    'Y' AS dss_is_active,
    CAST(NULL AS DATETIME2) AS dss_end_date
FROM new_records
```

---

### 5.6 PIT Table erstellen

ğŸ“„ **Vorlage:** [models/business_vault/pit_company.sql](../models/business_vault/pit_company.sql)

PIT (Point-in-Time) Tabellen fÃ¼r effiziente historische Abfragen.

```sql
{{ config(
    materialized='table',
    as_columnstore=false
) }}

WITH snapshot_dates AS (
    SELECT DISTINCT CAST(dss_load_date AS DATE) AS snapshot_date
    FROM {{ ref('sat_<entity>') }}
),

<entities> AS (
    SELECT DISTINCT hk_<entity>
    FROM {{ ref('hub_<entity>') }}
),

pit_base AS (
    SELECT 
        e.hk_<entity>,
        sd.snapshot_date
    FROM <entities> e
    CROSS JOIN snapshot_dates sd
),

sat_lookup AS (
    SELECT 
        pb.hk_<entity>,
        pb.snapshot_date,
        (
            SELECT TOP 1 s.hk_<entity>
            FROM {{ ref('sat_<entity>') }} s
            WHERE s.hk_<entity> = pb.hk_<entity>
              AND CAST(s.dss_load_date AS DATE) <= pb.snapshot_date
            ORDER BY s.dss_load_date DESC
        ) AS sat_<entity>_hk,
        (
            SELECT TOP 1 s.dss_load_date
            FROM {{ ref('sat_<entity>') }} s
            WHERE s.hk_<entity> = pb.hk_<entity>
              AND CAST(s.dss_load_date AS DATE) <= pb.snapshot_date
            ORDER BY s.dss_load_date DESC
        ) AS sat_<entity>_ldts
    FROM pit_base pb
)

SELECT * FROM sat_lookup
WHERE sat_<entity>_hk IS NOT NULL
```

---

## ğŸ“Š Mart View erstellen

### Szenario
Eine flache View fÃ¼r BI-Tools (Power BI, Excel) erstellen.

ğŸ“„ **Neue Datei:** `models/mart/v_<name>.sql`

**Beispiel: Aktuelle Firmendaten**

```sql
/*
 * Mart View: v_company_current
 * Schema: mart_project
 * 
 * Flache View mit aktuellen Firmendaten fÃ¼r Reporting.
 */

{{ config(
    materialized='view'
) }}

SELECT
    -- IDs (fÃ¼r Joins)
    h.hk_company,
    h.object_id,
    h.source_table,
    
    -- Stammdaten
    s.name AS company_name,
    s.street,
    s.citycode AS zip_code,
    s.city,
    s.country AS country_id,
    co.name AS country_name,
    
    -- Kontakt
    s.email,
    s.phone,
    s.mobile,
    s.website,
    
    -- Finanzen
    s.iban,
    s.bic,
    s.credit_rating,
    
    -- Rolle
    lr.role_code,
    r.role_name,
    
    -- Metadata
    s.dss_load_date AS last_updated,
    s.dss_record_source AS source_system

FROM {{ ref('hub_company') }} h

-- Aktuelle Satellite-Daten
INNER JOIN {{ ref('sat_company') }} s 
    ON h.hk_company = s.hk_company 
    AND s.dss_is_current = 'Y'

-- Rolle
LEFT JOIN {{ ref('link_company_role') }} lr 
    ON h.hk_company = lr.hk_company
LEFT JOIN {{ ref('ref_role') }} r 
    ON lr.role_code = r.role_code

-- Land
LEFT JOIN {{ ref('link_company_country') }} lc 
    ON h.hk_company = lc.hk_company
LEFT JOIN {{ ref('sat_country') }} co 
    ON lc.hk_country = co.hk_country 
    AND co.dss_is_current = 'Y'

-- Ghost Records ausschlieÃŸen
WHERE h.object_id > 0
```

**Konfiguration in dbt_project.yml:**

```yaml
models:
  datavault:
    mart:
      +schema: mart_project
      +materialized: view
```

**Deployment:**

```bash
dbt run --select v_company_current
```

---

## ğŸ§ª Tests hinzufÃ¼gen

### Test-Typen

| Test | Zweck | Beispiel |
|------|-------|----------|
| `not_null` | Spalte darf nicht NULL sein | Primary Keys, Business Keys |
| `unique` | Werte mÃ¼ssen eindeutig sein | Hash Keys in Hubs |
| `relationships` | FK-Beziehung validieren | Satellite â†’ Hub |
| `accepted_values` | Nur bestimmte Werte erlaubt | Status-Felder |

### Tests in schema.yml

ğŸ“„ **Datei:** [models/schema.yml](../models/schema.yml)

```yaml
models:
  - name: hub_<entity>
    columns:
      - name: hk_<entity>
        tests:
          - unique
          - not_null
      - name: <business_key>
        tests:
          - not_null

  - name: sat_<entity>
    columns:
      - name: hk_<entity>
        tests:
          - not_null
          - relationships:
              to: ref('hub_<entity>')
              field: hk_<entity>
      - name: hd_<entity>
        tests:
          - not_null
      - name: dss_is_current
        tests:
          - accepted_values:
              values: ['Y', 'N']
```

### Tests ausfÃ¼hren

```bash
# Alle Tests
dbt test

# Tests fÃ¼r bestimmtes Model
dbt test --select hub_company

# Tests fÃ¼r Tag
dbt test --select tag:hub
```

---

## ğŸš¢ Deployment Workflow

### GitHub Actions CI/CD Pipeline

Das Projekt verwendet **GitHub Actions** fÃ¼r automatisiertes Deployment. Der Self-hosted Runner lÃ¤uft auf der gleichen VM wie die Entwicklungsumgebung.

#### VerfÃ¼gbare Workflows

| Workflow | Trigger | Zweck |
|----------|---------|-------|
| **CI** | PR nach main/dev | Validierung (compile + test) |
| **Deploy Dev** | Push auf main / manual | Deployment nach Vault (Dev) |
| **Deploy Prod** | Tag v* / manual + Approval | Deployment nach Vault_Werkportal |
| **Docs** | Push auf main / manual | dbt docs â†’ GitHub Pages |

#### Workflow manuell ausfÃ¼hren

```bash
# Deploy Dev manuell triggern
gh workflow run deploy-dev.yml --ref main

# Deploy Prod manuell triggern (erfordert Approval!)
gh workflow run deploy-prod.yml --ref main -f target=werkportal

# Docs generieren
gh workflow run docs.yml --ref main
```

#### Workflow-Status prÃ¼fen

```bash
# Letzte Runs anzeigen
gh run list --limit 5

# Bestimmten Run beobachten
gh run watch <run-id>

# Logs eines fehlgeschlagenen Runs
gh run view <run-id> --log-failed
```

### Manuelles Deployment (Lokal)

Falls die Pipeline nicht verwendet werden soll:

```bash
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                    DEVELOPMENT                        â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Ã„nderungen entwickeln
dbt run --select <changed_models>

# 2. Tests lokal ausfÃ¼hren
dbt test --select <changed_models>

# 3. SQL prÃ¼fen
dbt compile --select <model>
cat target/compiled/datavault/models/path/to/model.sql

# 4. Git Commit & Push
git add .
git commit -m "feat: Add <feature>"
git push origin dev

# 5. Pull Request erstellen â†’ CI lÃ¤uft automatisch
gh pr create --base main --head dev --title "feat: <feature>"

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘              PRODUCTION (via CI/CD)                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 6. PR mergen â†’ Deploy Dev lÃ¤uft automatisch
gh pr merge <pr-number> --squash

# 7. FÃ¼r Prod: Tag erstellen oder manuell triggern
git tag v1.0.0 && git push origin v1.0.0
# ODER
gh workflow run deploy-prod.yml --ref main -f target=werkportal
# â†’ Approval in GitHub erforderlich!
```

### Manuelles Prod-Deployment (ohne CI/CD)

```bash
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘              PRODUCTION (manuell)                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. External Tables in Prod erstellen/aktualisieren
dbt run-operation stage_external_sources --target werkportal

# 2. Seeds laden (falls geÃ¤ndert)
dbt seed --target werkportal

# 3. Models deployen
dbt run --target werkportal

# 4. Tests in Prod
dbt test --target werkportal
```

### Full Refresh (Schema-Ã„nderungen)

```bash
# Bei SpaltenÃ¤nderungen: Full Refresh erforderlich!
dbt run --full-refresh --target werkportal
```

---

## ğŸ”§ Troubleshooting

### HÃ¤ufige Fehler

| Fehler | Ursache | LÃ¶sung |
|--------|---------|--------|
| `Column not found` | Spalte fehlt in External Table | `sources.yml` prÃ¼fen, `stage_external_sources` ausfÃ¼hren |
| `Columnstore not supported` | Azure SQL Basic Tier | `+as_columnstore: false` in Config |
| `Hash Diff changed unexpectedly` | Neue Spalte ohne Full Refresh | `dbt run --full-refresh` |
| `Duplicate key` | Unique-Constraint verletzt | Hash Key Berechnung prÃ¼fen |
| `Cross-database reference` | Hardcoded Database | `{{ target.database }}` verwenden |
| `Login timeout` | Azure Token abgelaufen | `az login` ausfÃ¼hren |

### Debug-Befehle

```bash
# Generiertes SQL anzeigen
dbt compile --select <model>
cat target/compiled/datavault/models/path/to/model.sql

# Logs prÃ¼fen
less logs/dbt.log

# Letzte Query
cat target/run/datavault/models/path/to/model.sql

# Verbindung testen
dbt debug
```

---

## âœ… Checklisten

### Neue Entity Checkliste

```
â–¡ External Table in sources.yml definiert
â–¡ Staging View erstellt (stg_<entity>.sql)
  â–¡ Hash Key berechnet
  â–¡ Hash Diff berechnet (falls Satellite)
  â–¡ Metadata-Spalten gemappt
â–¡ Hub erstellt (hub_<entity>.sql)
â–¡ Satellite erstellt (sat_<entity>.sql)
  â–¡ Post-Hook fÃ¼r dss_is_current
â–¡ Link erstellt (falls Beziehung)
â–¡ Tests in schema.yml hinzugefÃ¼gt
â–¡ dbt run-operation stage_external_sources
â–¡ dbt run --select stg_* hub_* sat_*
â–¡ dbt test
â–¡ Ghost Records erweitert (optional)
â–¡ Dokumentation aktualisiert
```

### Attribut hinzufÃ¼gen Checkliste

```
â–¡ Spalte in sources.yml hinzugefÃ¼gt
â–¡ Spalte in Staging View hinzugefÃ¼gt
â–¡ Spalte in Hash Diff (falls getrackt)
â–¡ Spalte in Satellite hinzugefÃ¼gt
â–¡ dbt run-operation stage_external_sources
â–¡ dbt run --full-refresh --select stg_* sat_*
â–¡ dbt test
```

### Pre-Deployment Checkliste

```
â–¡ Alle Tests lokal bestanden
â–¡ SQL kompiliert und geprÃ¼ft
â–¡ Keine hardcoded Datenbanknamen
â–¡ +as_columnstore: false gesetzt
â–¡ Hash-Separator ist '^^'
â–¡ Git committed und gepusht
â–¡ PR erstellt und CI erfolgreich âœ“
```

### CI/CD Troubleshooting

| Problem | LÃ¶sung |
|---------|--------|
| CI lÃ¤uft nicht | PrÃ¼fen ob Ã„nderungen in `models/`, `macros/`, etc. (Path Filter!) |
| Profile not found | `profile:` in dbt_project.yml muss mit profiles.yml Ã¼bereinstimmen |
| Runner offline | `sudo systemctl restart actions.runner.fellnerd-datavault-dbt.dbt-runner-vm` |
| Prod-Tests fehlen | `dbt seed --target werkportal` ausfÃ¼hren |
| Azure Login failed | Service Principal Secret ggf. abgelaufen, neu generieren |

---

## ğŸ“š WeiterfÃ¼hrende Dokumentation

| Dokument | Inhalt | Link |
|----------|--------|------|
| System-Dokumentation | Architektur, Komponenten | [SYSTEM.md](SYSTEM.md) |
| User-Dokumentation | Endanwender-Guide | [USER.md](USER.md) |
| Model Architecture | Datenmodell, ERD | [MODEL_ARCHITECTURE.md](MODEL_ARCHITECTURE.md) |
| Lessons Learned | Entscheidungen, Troubleshooting | [LESSONS_LEARNED.md](../LESSONS_LEARNED.md) |
| Copilot Instructions | KI-Assistenz Regeln | [copilot-instructions.md](../.github/copilot-instructions.md) |
| **CI/CD Plan** | Pipeline-Implementierung | [plan-githubActionsCiCd.prompt.prompt.md](../.github/prompts/plan-githubActionsCiCd.prompt.prompt.md) |
| **dbt Docs** | Generierte Dokumentation | [fellnerd.github.io/datavault-dbt](https://fellnerd.github.io/datavault-dbt/) |
| **GitHub Actions** | Pipeline-Runs | [Actions](https://github.com/fellnerd/datavault-dbt/actions) |

---

*Letzte Aktualisierung: 2025-12-27*
